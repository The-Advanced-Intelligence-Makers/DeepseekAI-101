# Day 6: Mixture of Experts (MoE)

Topics:

- [ ] Sparse activation.
- [ ] Expert routing.

Reading material

- [ ] [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
