{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Relative Policy Optimization (GRPO) - Explanation and Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy in Reinforcement Learning?\n",
    "\n",
    "A policy ($\\pi$) is a strategy that an agent follows to decide what action to take in a given state.\n",
    "The policy represents a probability distribution over possible outputs for a given input question ($\\text{q}$).\n",
    "\n",
    "### Policy Optimization\n",
    "\n",
    "Policy optimization techniques are used to find the best policy that maximizes the expected return.\n",
    "\n",
    "The goal of GRPO is to optimize the policy $\\pi_\\theta$ by comparing it with the old policy $\\pi_{\\theta_\\text{old}}$ and updating it based on group-wise feedback.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's now break down the GRPO algorithm step by step:\n",
    "\n",
    "*Scenario: Assume we have an AI system that generates responses to user questions.*\n",
    "- The system generates multiple possible answers (outputs) for each question.\n",
    "- We use a group of these responses to estimate which ones are relatively better.\n",
    "- Rewards are assigned based on user feedback or an evaluation metric.\n",
    "- Higher rewards are assigned to correct answers, and lower rewards to incorrect answers.\n",
    "\n",
    "*Example*\n",
    "\n",
    "**Question**: \"What is the capital of France?\"\n",
    "\n",
    "Possible outputs generated by the AI system {from $\\pi_{\\theta_\\text{old}}$}: [\"Paris\", \"London\", \"Berlin\", \"Madrid\"]\n",
    "\n",
    "in this case, the correct answer is \"Paris\".\n",
    "- \"Paris\" (correct, high reward)\n",
    "- \"London\" (incorrect, low reward)\n",
    "- \"Berlin\" (incorrect, low reward)\n",
    "- \"Madrid\" (incorrect, low reward)\n",
    "\n",
    "therefore, the rewards assigned to the outputs can be: [0.9, 0.1, 0.1, 0.1]\n",
    "<!-- Assigned rewards: {0.9, 0.1, 0.1, 0.1} -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Policy Outputs: ['Paris', 'London', 'Berlin', 'Madrid']\n",
      "Sampled Outputs: ['Berlin' 'Paris' 'London']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function that samples from the old policy outputs\n",
    "def sample_from_old_policy(old_policy_outputs, probabilities, sample_size=3):\n",
    "    \"\"\"\n",
    "    Function that samples from the old policy outputs, given the probabilities of each output\n",
    "    where:\n",
    "    old_policy_outputs: list of strings, the outputs of the old policy\n",
    "    probabilities: list of floats, the probability of each output\n",
    "    sample_size: int, the number of samples to draw.\n",
    "    \n",
    "    return the sampled outputs based on thier probabilities\n",
    "    \"\"\"\n",
    "    return np.random.choice(old_policy_outputs, size=sample_size, p=probabilities, replace=False)\n",
    "\n",
    "# Define old policy outputs and their probabilities\n",
    "old_policy_outputs = [\"Paris\", \"London\", \"Berlin\", \"Madrid\"]\n",
    "probabilities = [0.4, 0.2, 0.2, 0.2]  # Probability of selecting each output\n",
    "\n",
    "# Sample outputs\n",
    "sampled_outputs = sample_from_old_policy(old_policy_outputs, probabilities)\n",
    "print(\"Old Policy Outputs:\", old_policy_outputs)\n",
    "print(\"Sampled Outputs:\", sampled_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Advantage Function (Ai)\n",
    "\n",
    "An advantage function (${A_{i}}$) helps determine how much better or worse an output is relative to the others in the group. The advantage function is tyically calculated as the difference between the reward of the output and the average reward of the group.\n",
    "Given as:\n",
    "$$A_{i} = R_{i} - \\text{mean}(R_G)$$\n",
    "\n",
    "where:\n",
    "- $A_{i}$: Advantage of output $i$\n",
    "- $R_{i}$: Reward of output $i$\n",
    "- $R_{G}$: Rewards of all outputs in the group = $\\{r_{1}, r_{2}, ..., r_{G}\\}$\n",
    "- $\\text{mean}(R_G)$: Mean reward of the group\n",
    "\n",
    "In our example, the advantage function for each output would be:\n",
    "\n",
    "\n",
    "$$\\text{mean}(R_G) = \\frac{0.9 + 0.1 + 0.1 + 0.1}{4} = 0.3$$\n",
    "\n",
    "the advantage values would be:\n",
    "- \"Paris\": $0.9 - 0.3 = 0.6$\n",
    "- \"London\": $0.1 - 0.3 = -0.2$\n",
    "- \"Berlin\": $0.1 - 0.3 = -0.2$\n",
    "- \"Madrid\": $0.1 - 0.3 = -0.2$\n",
    "\n",
    "Advantage values: [0.6, -0.2, -0.2, -0.2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantages (Raw): [ 0.6 -0.2 -0.2 -0.2]\n"
     ]
    }
   ],
   "source": [
    "def compute_advantages_raw(rewards):\n",
    "    \"\"\"\n",
    "    Function that computes the advantages given the rewards\n",
    "    where:\n",
    "    rewards: list of floats, the rewards for each time step\n",
    "    \n",
    "    return the advantages for each time step\n",
    "    \"\"\"\n",
    "    # Compute the advantages\n",
    "    mean_reward = np.mean(rewards)\n",
    "    advantages = np.array(rewards) - mean_reward\n",
    "    return advantages\n",
    "\n",
    "# Define rewards\n",
    "rewards = [0.9, 0.1, 0.1, 0.1]\n",
    "advantages_raw = compute_advantages_raw(rewards)\n",
    "print(\"Advantages (Raw):\", advantages_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To fairly compare the outputs, we use the advantage values instead of the raw rewards.\n",
    "\n",
    "We normalize the advantage values by dividing them by their standard deviation to make the training process more stable.\n",
    "> ie after normalizing the advantage values, they are expected to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The advantage is now computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "( A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\dots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\dots, r_G\\})} )\n",
    "\\end{equation}\n",
    "\n",
    "*If the standard deviation is 0 (i.e., all rewards are the same), we set it to 1 to avoid division errors.*\n",
    "\n",
    "This results in a centered and scaled advantage score for each output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [0.9, 0.1, 0.1, 0.1]\n",
      "Advantages (Raw) [ 0.6 -0.2 -0.2 -0.2]\n",
      "Advantages Normalized: [1.732050807568877, -0.5773502691896257, -0.5773502691896257, -0.5773502691896257]\n"
     ]
    }
   ],
   "source": [
    "def compute_advantages(rewards):\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards) if np.std(rewards) > 0 else 1  # Avoid division by zero\n",
    "    advantages = [(r - mean_reward) / std_reward for r in rewards]\n",
    "    return advantages\n",
    "\n",
    "# Define rewards corresponding to sampled outputs\n",
    "rewards = [0.9, 0.1, 0.1, 0.1]  # Example rewards\n",
    "advantages = compute_advantages(rewards)\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Advantages (Raw)\", advantages_raw)\n",
    "print(\"Advantages Normalized:\", advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Advantages Output:\n",
    "\n",
    "- The first output (Paris) has a positive advantage (1.732), meaning it's much better than the group average.\n",
    "- The other outputs (London, Berlin, Madrid) have negative advantages (-0.577), indicating they are below average.\n",
    "- This advantage value is used to update the policy by rewarding better responses and penalizing worse ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Importance Sampling Ratio\n",
    "\n",
    "The importance sampling ratio helps measure how much the new policy differs from the old one.\n",
    "\n",
    "This is calculated by comparing the probabilities assigned to the outputs by the old and new policies.\n",
    "\n",
    "considering the output $o_i$ for question $q$, the importance sampling ratio is given as:\n",
    "\n",
    "$$\\text{ratio} = \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}$$\n",
    "\n",
    "where:\n",
    "- $\\pi_{\\theta}(o_i | q)$: Probability of output $o_i$ given question $q$ under the new policy $\\pi_{\\theta}$\n",
    "- $\\pi_{\\theta_{\\text{old}}}(o_i | q)$: Probability of output $o_i$ given question $q$ under the old policy $\\pi_{\\theta_{\\text{old}}}$\n",
    "- If ratio > 1, the new policy assigns a higher probability to this output than before.\n",
    "- If ratio < 1, the new policy assigns a lower probability than before.\n",
    "- This helps in adjusting the policy weights without making drastic changes.\n",
    "\n",
    "> *policy weights* are parameters that determine the probability distribution of outputs for a given input question. These weights are updated during training to improve the policy's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Sampling Ratios: [1.25, 1.0, 0.7499999999999999, 0.7499999999999999]\n"
     ]
    }
   ],
   "source": [
    "def compute_importance_sampling_ratios(new_policy_probs, old_policy_probs):\n",
    "    return [new / old for new, old in zip(new_policy_probs, old_policy_probs)]\n",
    "\n",
    "# Example probabilities under the new and old policies\n",
    "new_policy_probs = [0.5, 0.2, 0.15, 0.15]  # Hypothetical new policy probabilities\n",
    "old_policy_probs = [0.4, 0.2, 0.2, 0.2]  # Old policy probabilities\n",
    "\n",
    "ratios = compute_importance_sampling_ratios(new_policy_probs, old_policy_probs)\n",
    "print(\"Importance Sampling Ratios:\", ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Importance Sampling Ratios Output:\n",
    "- Importance Sampling Ratios: [1.25, 1.0, 0.75, 0.75]\n",
    "- The first output has a ratio of 1.25, meaning the new policy gives it a 25% higher probability than the old policy.\n",
    "- The second output remains unchanged (ratio = 1.0), meaning no change in probability.\n",
    "- The last two outputs have a ratio of 0.75, meaning they are assigned 25% lower probability under the new policy.\n",
    "- These adjustments help reinforce good responses and reduce the probability of bad responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clipping to Ensure Stability\n",
    "\n",
    "To prevent excessive policy updates, we **clip** the importance sampling ratio.\n",
    "- Clipping helps ensure that updates remain stable and controlled.\n",
    "- The formula for clipping is:\n",
    "\n",
    "clipped_ratio = clip(ratio, 1 - ε, 1 + ε)\n",
    "\n",
    "$$\\text{clipped\\_ratio} = \\text{clip}(ratio, 1 - \\epsilon, 1 + \\epsilon)$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon$: A small constant (e.g., 0.1) to limit the ratio's range.\n",
    "- If the ratio exceeds (1 + ε), it is capped at (1 + ε).\n",
    "- Similarly, if the ratio falls below (1 - ε), it is raised to (1 - ε).\n",
    "- This avoids drastic changes in policy probability, ensuring smooth learning.\n",
    "\n",
    "<!-- We clip the ratio to avoid making drastic updates: -->\n",
    "<!-- \n",
    "clipped_ratio = clip(ratio, 1 - ε, 1 + ε)\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- Why? If we allow unbounded updates, training becomes unstable.\n",
    "- Clipping ensures that probability changes remain within a safe range, preventing excessive updates that could cause instability. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped Ratios: [1.2, 1.0, 0.8, 0.8]\n"
     ]
    }
   ],
   "source": [
    "def clip_ratios(ratios, epsilon=0.2):\n",
    "    return [max(1 - epsilon, min(r, 1 + epsilon)) for r in ratios]\n",
    "\n",
    "# Clipping importance sampling ratios\n",
    "clipped_ratios = clip_ratios(ratios)\n",
    "print(\"Clipped Ratios:\", clipped_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Clipped Ratios Output:\n",
    "- Clipping ensures that probability adjustments remain within the range [0.8, 1.2].\n",
    "- This prevents overly aggressive updates, keeping training stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Policy Objective Function\n",
    "\n",
    "The policy objective function is then used to update the policy weights through gradient ascent. This function combines the clipped surrogate objective and the advantage function to update the policy weights.\n",
    "\n",
    "The policy objective function ensures that policy updates lead to improved performance.\n",
    "\n",
    "The clipped surrogate objective is defined as:\n",
    "\n",
    "$$L_{\\text{clip}}(\\theta) = \\text{min}(ratio \\cdot A, \\text{clipped\\_ratio} \\cdot A)$$\n",
    "\n",
    "where:\n",
    "- $L_{\\text{clip}}(\\theta)$: Clipped surrogate objective function for policy update\n",
    "- $ratio$: Importance sampling ratio\n",
    "- $A$: Advantage function\n",
    "- $\\text{clipped\\_ratio}$: Clipped importance sampling ratio\n",
    "- The minimum of the two terms is taken to balance policy updates.\n",
    "  - If the ratio leads to a large update, the clipped ratio is used instead to stabilize learning.\n",
    "  - If the ratio leads to a small update, the original ratio is used.\n",
    "- This prevents overly aggressive policy updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Objective Values: [2.078460969082652, -0.5773502691896257, -0.4618802153517006, -0.4618802153517006]\n"
     ]
    }
   ],
   "source": [
    "def compute_policy_objective(clipped_ratios, advantages):\n",
    "    objective_values = [min(r * A, r * A) for r, A in zip(clipped_ratios, advantages)]\n",
    "    return objective_values\n",
    "\n",
    "# Compute policy objective\n",
    "policy_objective = compute_policy_objective(clipped_ratios, advantages)\n",
    "print(\"Policy Objective Values:\", policy_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Policy Objective Values:\n",
    "- Positive values indicate an increase in probability for that action.\n",
    "- Negative values suggest reducing the probability of selecting that action.\n",
    "- The clipping ensures that no single update is too large, preventing instability.\n",
    "\n",
    "> instability is defined by poor convergence or oscillations in the training process, which can hinder learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: KL Divergence Regularization\n",
    "\n",
    "Divergence is a measure of how much one probability distribution differs from another.\n",
    "Divergence is used to penalize large deviations between the new policy and the old policy.\n",
    "\n",
    "Divergence regularization helps stabilize training by limiting policy updates. This can be done by adding a divergence penalty to the policy objective function.\n",
    "\n",
    "KL divergence measures the difference between the new policy and the old policy.\n",
    "Kullback-Leibler(KL) divergence, a measure of how one probability distribution diverges from a second, expected probability distribution denoted as ${D_{\\text{KL}}(P\\parallel Q)}$.\n",
    "\n",
    "The standard formula for KL divergence is:\n",
    "\n",
    "$$D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ \\log \\left({\\frac {\\ P(x)\\ }{Q(x)}}\\right).$$\n",
    "\n",
    "where:\n",
    "- $P(x)$: Probability of event $x$ under the new policy\n",
    "- $Q(x)$: Probability of event $x$ under the old policy \n",
    "- The sum is taken over all possible events $x$ in the sample space $\\mathcal{X}$.\n",
    "- A high KL divergence suggests a large difference between the policies, requiring regularization.\n",
    "\n",
    "> also referred to as relative entropy, information divergence, information gain and I divergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.025815408455028527\n"
     ]
    }
   ],
   "source": [
    "def compute_kl_divergence(old_policy_probs, new_policy_probs):\n",
    "    kl_div = sum(o * np.log(o / n) for o, n in zip(old_policy_probs, new_policy_probs) if o > 0 and n > 0)\n",
    "    return kl_div\n",
    "\n",
    "kl_divergence = compute_kl_divergence(old_policy_probs, new_policy_probs)\n",
    "print(\"KL Divergence:\", kl_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "the KL divergence in GRPO is given as:\n",
    "\n",
    "$$\n",
    "D_{KL}(\\pi_\\theta \\mid\\mid \\pi_{ref}) = \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{ref}(o_i \\mid q)} - \\log \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{ref}(o_i \\mid q)} - 1\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\pi_\\theta(o_i \\mid q)$: Probability of output $o_i$ given question $q$ under the new policy $\\pi_{\\theta}$\n",
    "- $\\pi_{ref}(o_i \\mid q)$: Probability of output $o_i$ given question $q$ under the reference policy $\\pi_{ref}$\n",
    "- The KL divergence is calculated for each output and summed over all outputs.\n",
    "- The regularization term penalizes large KL divergence, ensuring policy updates are controlled.\n",
    "\n",
    "> reference policy $\\pi_{ref}$ is either the old policy $\\pi_{\\theta_{\\text{old}}}$ or a fixed policy used for stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Penalty (Reference): 0.17040418085046793\n",
      "KL Divergence (Old vs New): 0.11444607307731469\n",
      "GRPO KL Divergence: 0.10222059358935232\n"
     ]
    }
   ],
   "source": [
    "def compute_kl_penalty(old_policy_probs, ref_policy_probs):\n",
    "    kl_penalty = sum((new / ref) - np.log(new / ref) - 1 for new, ref in zip(old_policy_probs, ref_policy_probs) if new > 0 and ref > 0)\n",
    "    return kl_penalty\n",
    "\n",
    "\n",
    "def compute_grpo_kl_divergence(old_policy_probs, ref_policy_probs):\n",
    "    kl_div= sum((new / ref) - np.log(new / ref) - 1 for new, ref in zip(old_policy_probs, ref_policy_probs) if new > 0 and ref > 0)\n",
    "    return kl_div\n",
    "\n",
    "# Reference policy probabilities (could be same as old policy or a smoothed version)\n",
    "\n",
    "kl_grpo_divergence = compute_grpo_kl_divergence(old_policy_probs, new_policy_probs)\n",
    "print(\"GRPO KL Divergence:\", kl_grpo_divergence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of KL Divergence:\n",
    "- KL Divergence = 0.025815408455028527 means the new policy is slightly different from the old one.\n",
    "- A low value suggests the policy update is stable and not drastically changing behavior.\n",
    "- A high KL divergence would indicate that the new policy is deviating significantly, which could lead to instability.\n",
    "- Regularization helps by penalizing large deviations, ensuring smooth learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Final GRPO Objective\n",
    " - The final objective function combines the clipped loss and KL divergence penalty:\n",
    "   J_GRPO(θ) = mean(L) - β * D_KL\n",
    "\n",
    " Now, let's implement each of these steps in Python, one by one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J_{GRPO}(\\theta) = E\\left[ \\sum_{i=1}^G \\left( \\min\\left( \\frac{\\pi_{\\theta_{old}}(o_i \\mid q)}{\\pi_\\theta(o_i \\mid q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta_{old}}(o_i \\mid q)}{\\pi_\\theta(o_i \\mid q)}, 1-\\varepsilon, 1+\\varepsilon \\right) A_i \\right) - \\beta D_{KL}(\\pi_\\theta \\mid\\mid \\pi_{ref}) \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
