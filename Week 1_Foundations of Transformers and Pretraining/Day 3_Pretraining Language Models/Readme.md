# Day 3: Pretraining Language Models

Topics:

- [ ] Autoregressive vs. masked language modeling.
- [ ] Tokenization (Byte Pair Encoding, WordPiece).
- [ ] Subword tokenization.(optional)

Reading material:

- [ ]  [Language Models are few-shot learners](https://arxiv.org/abs/2005.14165)
- [ ]  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
