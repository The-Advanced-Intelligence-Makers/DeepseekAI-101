# Week 1: Foundations of Transformers and Pretraining

Topics:

- Self-attention mechanism.
- Multi-head attention.
- Positional encoding.
- Encoder-decoder architecture.
- Feed-forward networks.
- Layer normalization.
- Autoregressive vs. masked language modeling.
- Tokenization (Byte Pair Encoding, WordPiece).
- Fine-tuning on downstream tasks.
- Parameter-efficient fine-tuning (e.g., LoRA).
- Model parallelism.
- Mixed precision training.
