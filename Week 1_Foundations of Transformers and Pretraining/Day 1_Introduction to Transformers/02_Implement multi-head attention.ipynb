{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "In the previous notebook, we saw that self-attention helps focus on the most relevant words when processing a sentence. But what if there are multiple ways to determine relevance?\n",
    "\n",
    "Consider the previous example:\n",
    "\n",
    "The cat sat on the mat.\n",
    "\n",
    "If we ask, \"Where did the cat sit?\", the word \"mat\" is important. However, there are other relationships in the sentence:\n",
    "\n",
    "- The word \"cat\" is related to \"sat\" (because the cat is the subject of the action).\n",
    "- The word \"on\" is also relevant because it describes the spatial relationship between \"sat\" and \"mat\".\n",
    "\n",
    "With only one attention head, the model might only capture one of these relationships at a time.\n",
    "\n",
    "Solution:\n",
    "Each attention head looks at the sentence from a different perspective. This way, the model can capture multiple relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the multi-head attention layer is a sequence of vectors. Each vector is the output of the previous layer (e.g., the word embeddings). The multi-head attention layer consists of multiple attention heads. Each head applies a different learned linear transformation to the input vectors. The outputs of the different heads are concatenated and linearly transformed to produce the final output of the multi-head attention layer.</p>\n",
    "<img src=\"mha_img_original.png\" width=\"500\" style=\"background-color:white;\">\n",
    "\n",
    "in simple terms we apply multiple *scaled dot-product attentions* in parallel\n",
    "\n",
    "<img src=\"mha_visualization-930x1030.png\" width=\"500\" style=\"background-color:white;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: single head attention\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]  # Dimension of key vectors\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Compute scaled dot-product attention scores\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Apply softmax\n",
    "    output = np.dot(attention_weights, V)  # Weighted sum of values\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each head in multi-head attention is responsible for computing scaled dot-product attention in a lower-dimensional subspace.\n",
    "Mathematically, a single head is computed as:\n",
    "\n",
    "$$ head_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$\n",
    "\n",
    "where:\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$ are learned linear transformations\n",
    "\n",
    "This means each head has its own learnable weight matrices $(W_i^Q, W_i^K, W_i^V)$, which project the input queries, keys, and values into lower-dimensional subspaces before applying attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
