# Intro to Transformers

Topics to be covered

- [X] Self-attention mechanism.
- [ ] Multi-head attention.
- [ ] Positional encoding.

Reading material:

- [ ] [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [ ] [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
